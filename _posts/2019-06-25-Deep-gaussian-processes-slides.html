<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2019-01-11">
  <title>Deep Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          TeX: {
            extensions: ["color.js"]
          }
        });
      </script>
      <script>
  
  function setDivs(group) {
    var frame = document.getElementById("range-".concat(group)).value
    slideIndex = parseInt(frame)
    showDivs(slideIndex, group);
  }
  
  function plusDivs(n, group) {
    showDivs(slideIndex += n, group);
    document.setElementById("range-".concat(group)) = slideIndex
  }
  
  function showDivs(n,group) {
    var i;
    var x = document.getElementsByClassName(group);
    if (n > x.length) {slideIndex = 1}    
    if (n < 1) {slideIndex = x.length}
    for (i = 0; i < x.length; i++) {
       x[i].style.display = "none";  
    }
    x[slideIndex-1].style.display = "block";  
  }
      </script>
</head>
<body>
\[\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Deep Gaussian Processes</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2019-01-11</time></p>
  <p class="venue" style="text-align:center">MLSS, Stellenbosch, South Africa</p>
</section>

<section class="slide level3">

<!--define{draft}-->
<!-- Front matter -->
<!--Back matter-->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!-- SECTION Introduction -->
</section>
<section id="section" class="slide level3">
<h3></h3>
<div class="centered" style="">
<img class="vertical-align:middle" src="../slides/diagrams/Planck_CMB.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<div style="fontsize:120px;vertical-align:middle">
<img src="../slides/diagrams/earth_PNG37.png" width="20%" style="display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;"><span class="math inline">\(=f\Bigg(\)</span><img src="../slides/diagrams/Planck_CMB.png"  width="50%" style="display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;"><span class="math inline">\(\Bigg)\)</span>
</div>
<!-- SECTION Low Rank Gaussian Processes -->
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="approximations" class="slide level3">
<h3>Approximations</h3>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-1.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="approximations-1" class="slide level3">
<h3>Approximations</h3>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-2.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="approximations-2" class="slide level3">
<h3>Approximations</h3>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-3.png" width="45%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="approximations-3" class="slide level3">
<h3>Approximations</h3>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-4.png" width="45%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<p><span style="text-align:right"></span></p>
</section>
<section id="low-rank-motivation" class="slide level3">
<h3>Low Rank Motivation</h3>
<ul>
<li>Inference in a GP has the following demands:</li>
</ul>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Complexity:</td>
<td style="text-align: left;"><span class="math inline">\(\bigO(\numData^3)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: right;">Storage:</td>
<td style="text-align: left;"><span class="math inline">\(\bigO(\numData^2)\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>Inference in a low rank GP has the following demands:</li>
</ul>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Complexity:</td>
<td style="text-align: left;"><span class="math inline">\(\bigO(\numData\numInducing^2)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: right;">Storage:</td>
<td style="text-align: left;"><span class="math inline">\(\bigO(\numData\numInducing)\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(\numInducing\)</span> is a user chosen parameter.</p>
<p><small><span class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani (n.d.)</span>,<span class="citation" data-cites="Quinonero:unifying05">Quiñonero Candela and Rasmussen (2005)</span>,<span class="citation" data-cites="Lawrence:larger07">Lawrence (n.d.)</span>,<span class="citation" data-cites="Titsias:variational09">Titsias (n.d.)</span>,<span class="citation" data-cites="Thang:unifying17">Bui et al. (2017)</span></small></p>
<p><span style="text-align:right"></span> ### Variational Compression</p>
<ul>
<li>Inducing variables are a compression of the real observations.</li>
<li>They are like pseudo-data. They can be in space of <span class="math inline">\(\mappingFunctionVector\)</span> or a space that is related through a linear operator <span class="citation" data-cites="Alvarez:efficient10">(Álvarez et al., 2010)</span> — e.g. a gradient or convolution.</li>
</ul>
</section>
<section id="variational-compression-ii" class="slide level3">
<h3>Variational Compression II</h3>
<ul>
<li>Introduce <em>inducing</em> variables.</li>
<li>Compress information into the inducing variables and avoid the need to store all the data.</li>
<li>Allow for scaling e.g. stochastic variational <span class="citation" data-cites="Hensman:bigdata13">Hensman et al. (n.d.)</span> or parallelization <span class="citation" data-cites="Gal:distributed14">Gal et al. (n.d.)</span>,<span class="citation" data-cites="Dai:gpu14">Dai et al. (2014)</span>, <span class="citation" data-cites="Seeger:auto17">Seeger et al. (2017)</span></li>
</ul>
<!--include{_gp/includes/larger-factorize.md}-->
<p><span style="text-align:right"></span></p>
</section>
<section id="nonparametric-gaussian-processes" class="slide level3">
<h3>Nonparametric Gaussian Processes</h3>
<ul>
<li><p>We’ve seen how we go from parametric to non-parametric.</p></li>
<li><p>The limit implies infinite dimensional <span class="math inline">\(\mappingVector\)</span>.</p></li>
<li><p>Gaussian processes are generally non-parametric: combine data with covariance function to get model.</p></li>
<li><p>This representation <em>cannot</em> be summarized by a parameter vector of a fixed size.</p></li>
</ul>
</section>
<section id="the-parametric-bottleneck" class="slide level3">
<h3>The Parametric Bottleneck</h3>
<ul>
<li><p>Parametric models have a representation that does not respond to increasing training set size.</p></li>
<li><p>Bayesian posterior distributions over parameters contain the information about the training data.</p>
<ul>
<li><p>Use Bayes’ rule from training data, <span class="math inline">\(p\left(\mappingVector|\dataVector, \inputMatrix\right)\)</span>,</p></li>
<li><p>Make predictions on test data <span class="math display">\[p\left(\dataScalar_*|\inputMatrix_*, \dataVector, \inputMatrix\right) = \int
      p\left(\dataScalar_*|\mappingVector,\inputMatrix_*\right)p\left(\mappingVector|\dataVector,
        \inputMatrix)\text{d}\mappingVector\right).\]</span></p></li>
</ul></li>
<li><p><span class="math inline">\(\mappingVector\)</span> becomes a bottleneck for information about the training set to pass to the test set.</p></li>
<li><p>Solution: increase <span class="math inline">\(\numBasisFunc\)</span> so that the bottleneck is so large that it no longer presents a problem.</p></li>
<li><p>How big is big enough for <span class="math inline">\(\numBasisFunc\)</span>? Non-parametrics says <span class="math inline">\(\numBasisFunc \rightarrow \infty\)</span>.</p></li>
</ul>
</section>
<section id="the-parametric-bottleneck-1" class="slide level3">
<h3>The Parametric Bottleneck</h3>
<ul>
<li>Now no longer possible to manipulate the model through the standard parametric form.</li>
</ul>
<div class="fragment">
<ul>
<li>However, it <em>is</em> possible to express <em>parametric</em> as GPs: <span class="math display">\[\kernelScalar\left(\inputVector_i,\inputVector_j\right)=\basisFunction_:\left(\inputVector_i\right)^\top\basisFunction_:\left(\inputVector_j\right).\]</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>These are known as degenerate covariance matrices.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Their rank is at most <span class="math inline">\(\numBasisFunc\)</span>, non-parametric models have full rank covariance matrices.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Most well known is the “linear kernel”, <span class="math inline">\(\kernelScalar(\inputVector_i, \inputVector_j) = \inputVector_i^\top\inputVector_j\)</span>.</li>
</ul>
</div>
</section>
<section id="making-predictions" class="slide level3">
<h3>Making Predictions</h3>
<ul>
<li>For non-parametrics prediction at new points <span class="math inline">\(\mappingFunctionVector_*\)</span> is made by conditioning on <span class="math inline">\(\mappingFunctionVector\)</span> in the joint distribution.</li>
</ul>
<div class="fragment">
<ul>
<li>In GPs this involves combining the training data with the covariance function and the mean function.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Parametric is a special case when conditional prediction can be summarized in a <em>fixed</em> number of parameters.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Complexity of parametric model remains fixed regardless of the size of our training data set.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>For a non-parametric model the required number of parameters grows with the size of the training data.</li>
</ul>
</div>
</section>
<section id="augment-variable-space" class="slide level3">
<h3>Augment Variable Space</h3>
<ul>
<li>Augment variable space with inducing observations, <span class="math inline">\(\inducingVector\)</span> <span class="math display">\[
\begin{bmatrix}
\mappingFunctionVector\\
\inducingVector
\end{bmatrix} \sim \gaussianSamp{\zerosVector}{\kernelMatrix}
\]</span> with <span class="math display">\[
\kernelMatrix =
\begin{bmatrix}
\Kff &amp; \Kfu \\
\Kuf &amp; \Kuu
\end{bmatrix}
\]</span></li>
</ul>
</section>
<section id="joint-density" class="slide level3">
<h3>Joint Density</h3>
<p><span class="math display">\[
p(\mappingFunctionVector, \inducingVector) = p(\mappingFunctionVector| \inducingVector) p(\inducingVector)
\]</span> to augment our model <span class="math display">\[
\dataScalar(\inputVector) = \mappingFunction(\inputVector) + \noiseScalar,
\]</span> giving <span class="math display">\[
p(\dataVector) = \int p(\dataVector|\mappingFunctionVector) p(\mappingFunctionVector) \text{d}\mappingFunctionVector,
\]</span> where for the independent case we have <span class="math inline">\(p(\dataVector | \mappingFunctionVector) = \prod_{i=1}^\numData p(\dataScalar_i|\mappingFunction_i)\)</span>.</p>
</section>
<section id="auxilliary-variables" class="slide level3">
<h3>Auxilliary Variables</h3>
<p><span class="math display">\[
p(\dataVector) = \int p(\dataVector|\mappingFunctionVector) p(\mappingFunctionVector|\inducingVector)  p(\inducingVector)  \text{d}\inducingVector \text{d}\mappingFunctionVector.
\]</span> Integrating over <span class="math inline">\(\mappingFunctionVector\)</span> <span class="math display">\[
p(\dataVector) = \int p(\dataVector|\inducingVector)   p(\inducingVector)  \text{d}\inducingVector.
\]</span></p>
</section>
<section id="parametric-comparison" class="slide level3">
<h3>Parametric Comparison</h3>
<p><span class="math display">\[
\dataScalar(\inputVector) = \weightVector^\top\basisVector(\inputVector) + \noiseScalar
\]</span></p>
<p><span class="math display">\[
p(\dataVector) = \int p(\dataVector|\weightVector) p(\weightVector) \text{d} \weightVector
\]</span></p>
<p><span class="math display">\[
p(\dataVector^*|\dataVector) = \int p(\dataVector^*|\weightVector) p(\weightVector|\dataVector) \text{d} \weightVector
\]</span></p>
</section>
<section id="new-form" class="slide level3">
<h3>New Form</h3>
<p><span class="math display">\[
p(\dataVector^*|\dataVector) = \int p(\dataVector^*|\inducingVector) p(\inducingVector|\dataVector) \text{d} \inducingVector
\]</span></p>
<ul>
<li><p>but <span class="math inline">\(\inducingVector\)</span> is not a <em>parameter</em></p></li>
<li><p>Unfortunately computing <span class="math inline">\(p(\dataVector|\inducingVector)\)</span> is intractable</p></li>
</ul>
<p><span style="text-align:right"></span></p>
</section>
<section id="variational-bound-on-pdatavector-inducingvector" class="slide level3">
<h3>Variational Bound on <span class="math inline">\(p(\dataVector |\inducingVector)\)</span></h3>
<p><span class="math display">\[
\begin{aligned}
    \log p(\dataVector|\inducingVector) &amp; = \log \int p(\dataVector|\mappingFunctionVector) p(\mappingFunctionVector|\inducingVector) \text{d}\mappingFunctionVector\\ &amp; = \int q(\mappingFunctionVector) \log \frac{p(\dataVector|\mappingFunctionVector) p(\mappingFunctionVector|\inducingVector)}{q(\mappingFunctionVector)}\text{d}\mappingFunctionVector + \KL{q(\mappingFunctionVector)}{p(\mappingFunctionVector|\dataVector, \inducingVector)}.
\end{aligned}
\]</span></p>
</section>
<section id="choose-form-for-qcdot" class="slide level3">
<h3>Choose form for <span class="math inline">\(q(\cdot)\)</span></h3>
<ul>
<li>Set <span class="math inline">\(q(\mappingFunctionVector)=p(\mappingFunctionVector|\inducingVector)\)</span>, <span class="math display">\[
  \log p(\dataVector|\inducingVector) \geq \int p(\mappingFunctionVector|\inducingVector) \log p(\dataVector|\mappingFunctionVector)\text{d}\mappingFunctionVector.
  \]</span> <span class="math display">\[
  p(\dataVector|\inducingVector) \geq \exp \int p(\mappingFunctionVector|\inducingVector) \log p(\dataVector|\mappingFunctionVector)\text{d}\mappingFunctionVector.
  \]</span> <span style="text-align:right"></span></li>
</ul>
</section>
<section id="optimal-compression-in-inducing-variables" class="slide level3">
<h3>Optimal Compression in Inducing Variables</h3>
<ul>
<li><p>Maximizing lower bound minimizes the KL divergence (information gain): <span class="math display">\[
  \KL{p(\mappingFunctionVector|\inducingVector)}{p(\mappingFunctionVector|\dataVector, \inducingVector)} = \int p(\mappingFunctionVector|\inducingVector) \log \frac{p(\mappingFunctionVector|\inducingVector)}{p(\mappingFunctionVector|\dataVector, \inducingVector)}\text{d}\inducingVector
  \]</span></p></li>
<li><p>This is minimized when the information stored about <span class="math inline">\(\dataVector\)</span> is stored already in <span class="math inline">\(\inducingVector\)</span>.</p></li>
<li>The bound seeks an <em>optimal compression</em> from the <em>information gain</em> perspective.</li>
<li><p>If <span class="math inline">\(\inducingVector = \mappingFunctionVector\)</span> bound is exact (<span class="math inline">\(\mappingFunctionVector\)</span> <span class="math inline">\(d\)</span>-separates <span class="math inline">\(\dataVector\)</span> from <span class="math inline">\(\inducingVector\)</span>).</p></li>
</ul>
</section>
<section id="choice-of-inducing-variables" class="slide level3">
<h3>Choice of Inducing Variables</h3>
<ul>
<li>Free to choose whatever heuristics for the inducing variables.</li>
<li>Can quantify which heuristics perform better through checking lower bound.</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
\mappingFunctionVector\\
\inducingVector
\end{bmatrix} \sim \gaussianSamp{\zerosVector}{\kernelMatrix}
\]</span> with <span class="math display">\[
\kernelMatrix =
\begin{bmatrix}
\Kff &amp; \Kfu \\
\Kuf &amp; \Kuu
\end{bmatrix}
\]</span></p>
</section>
<section id="variational-compression" class="slide level3">
<h3>Variational Compression</h3>
<ul>
<li>Inducing variables are a compression of the real observations.</li>
<li>They are like pseudo-data. They can be in space of <span class="math inline">\(\mappingFunctionVector\)</span> or a space that is related through a linear operator <span class="citation" data-cites="Alvarez:efficient10">(Álvarez et al., 2010)</span> — e.g. a gradient or convolution.</li>
</ul>
</section>
<section id="variational-compression-ii-1" class="slide level3">
<h3>Variational Compression II</h3>
<ul>
<li>Resulting algorithms reduce computational complexity.</li>
<li>Also allow deployment of more standard scaling techniques.</li>
<li>E.g. Stochastic variational inference <span class="citation" data-cites="Hoffman:stochastic12">Hoffman et al. (2012)</span></li>
<li>Allow for scaling e.g. stochastic variational <span class="citation" data-cites="Hensman:bigdata13">Hensman et al. (n.d.)</span> or parallelization <span class="citation" data-cites="Gal:distributed14">Gal et al. (n.d.)</span>,<span class="citation" data-cites="Dai:gpu14">Dai et al. (2014)</span>, <span class="citation" data-cites="Seeger:auto17">Seeger et al. (2017)</span></li>
</ul>
<!--include{_gp/includes/larger-graph-intro.md}-->
<p><span style="text-align:right"></span></p>
</section>
<section id="full-gaussian-process-fit" class="slide level3">
<h3>Full Gaussian Process Fit</h3>
<object class="svgplot " align data="../slides/diagrams/gp/sparse-demo-full-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="inducing-variable-fit" class="slide level3">
<h3>Inducing Variable Fit</h3>
<object class="svgplot " align data="../slides/diagrams/gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="inducing-variable-param-optimize" class="slide level3">
<h3>Inducing Variable Param Optimize</h3>
<object class="svgplot " align data="../slides/diagrams/gp/sparse-demo-constrained-inducing-6-learned-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="inducing-variable-full-optimize" class="slide level3">
<h3>Inducing Variable Full Optimize</h3>
<object class="svgplot " align data="../slides/diagrams/gp/sparse-demo-unconstrained-inducing-6-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="eight-optimized-inducing-variables" class="slide level3">
<h3>Eight Optimized Inducing Variables</h3>
<object class="svgplot " align data="../slides/diagrams/gp/sparse-demo-sparse-inducing-8-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="full-gaussian-process-fit-1" class="slide level3">
<h3>Full Gaussian Process Fit</h3>
<object class="svgplot " align data="../slides/diagrams/gp/sparse-demo-full-gp.svg" style="vertical-align:middle;">
</object>
<p><span style="text-align:right"></span></p>
</section>
<section id="leads-to-other-approximations" class="slide level3">
<h3>Leads to Other Approximations …</h3>
<ul>
<li>Let’s be explicity about storing approximate posterior of <span class="math inline">\(\inducingVector\)</span>, <span class="math inline">\(q(\inducingVector)\)</span>.</li>
<li>Now we have <span class="math display">\[p(\dataVector^*|\dataVector) = \int p(\dataVector^*| \inducingVector) q(\inducingVector | \dataVector) \text{d} \inducingVector\]</span></li>
</ul>
</section>
<section id="leads-to-other-approximations-1" class="slide level3">
<h3>Leads to Other Approximations …</h3>
<ul>
<li>Inducing variables look a lot like regular parameters.</li>
<li><em>But</em>: their dimensionality does not need to be set at design time.</li>
<li>They can be modified arbitrarily at run time without effecting the model likelihood.</li>
<li>They only effect the quality of compression and the lower bound.</li>
</ul>
</section>
<section id="in-gps-for-big-data" class="slide level3">
<h3>In GPs for Big Data</h3>
<ul>
<li>Exploit the resulting factorization … <span class="math display">\[p(\dataVector^*|\dataVector) = \int p(\dataVector^*| \inducingVector) q(\inducingVector | \dataVector) \text{d} \inducingVector\]</span></li>
<li>The distribution now <em>factorizes</em>: <span class="math display">\[p(\dataVector^*|\dataVector) = \int \prod_{i=1}^{\numData^*}p(\dataScalar^*_i| \inducingVector) q(\inducingVector | \dataVector) \text{d} \inducingVector\]</span></li>
<li>This factorization can be exploited for stochastic variational inference <span class="citation" data-cites="Hoffman:stochastic12">(Hoffman et al., 2012)</span>.</li>
</ul>
</section>
<section id="nonparametrics-for-very-large-data-sets" class="slide level3">
<h3>Nonparametrics for Very Large Data Sets</h3>
<center>
Modern data availability
</center>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/ml/house_price_country.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="nonparametrics-for-very-large-data-sets-1" class="slide level3">
<h3>Nonparametrics for Very Large Data Sets</h3>
<center>
Proxy for index of deprivation?
</center>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/ml/house_price_peak_district.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="nonparametrics-for-very-large-data-sets-2" class="slide level3">
<h3>Nonparametrics for Very Large Data Sets</h3>
<center>
Actually index of deprivation is a proxy for this …
</center>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/ml/house_price_peak_district.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-2" class="slide level3">
<h3></h3>

<table>
<tr>
<td width>
<span style="text-align:left"><span class="citation" data-cites="Hensman:bigdata13">(Hensman et al., n.d.)</span></span>
</td>
<td width>
<span style="text-align:right"><img class="" src="../slides/diagrams/people/2013_03_28_180606.JPG" width="1.5cm" align="" style="background:none; border:none; box-shadow:none; position:absolute; clip:rect(100px,2369px,2200px,1370px);vertical-align:middle"></span>
</td>
</tr>
</table>
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/244_1_clip.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
<a href="http://auai.org/uai2013/prints/papers/244.pdf" class="uri">http://auai.org/uai2013/prints/papers/244.pdf</a>
</center>
</section>
<section id="section-3" class="slide level3">
<h3></h3>
<table>
<tr>
<td width>
<span style="text-align:left"><span class="citation" data-cites="Hensman:bigdata13">(Hensman et al., n.d.)</span></span>
</td>
<td width>
<span style="text-align:right"><img class="" src="../slides/diagrams/people/2013_03_28_180606.JPG" width="1.5cm" align="" style="background:none; border:none; box-shadow:none; position:absolute; clip:rect(100px,2369px,2200px,1370px);vertical-align:middle"></span>
</td>
</tr>
</table>
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/244_6_clip.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
<a href="http://auai.org/uai2013/prints/papers/244.pdf" class="uri">http://auai.org/uai2013/prints/papers/244.pdf</a>
</center>
</section>
<section id="modern-review" class="slide level3">
<h3>Modern Review</h3>
<ul>
<li><p><em>A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation</em> <span class="citation" data-cites="Thang:unifying17">Bui et al. (2017)</span></p></li>
<li><p><em>Deep Gaussian Processes and Variational Propagation of Uncertainty</em> <span class="citation" data-cites="Damianou:thesis2015">Damianou (2015)</span></p></li>
</ul>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="section-4" class="slide level3">
<h3></h3>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/mackay-baby.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="structure-of-priors" class="slide level3">
<h3>Structure of Priors</h3>
<p>MacKay: NeurIPS Tutorial 1997 “Have we thrown out the baby with the bathwater?” <span class="citation" data-cites="MacKay:gpintroduction98">(Published as MacKay, n.d.)</span></p>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="deep-neural-network" class="slide level3">
<h3>Deep Neural Network</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/deep-nn1.svg" style="vertical-align:middle;">
</object>
</section>
<section id="deep-neural-network-1" class="slide level3">
<h3>Deep Neural Network</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/deep-nn2.svg" style="vertical-align:middle;">
</object>
</section>
<section id="mathematically" class="slide level3">
<h3>Mathematically</h3>
<p><span class="math display">\[
\begin{align}
    \hiddenVector_{1} &amp;= \basisFunction\left(\mappingMatrix_1 \inputVector\right)\\
    \hiddenVector_{2} &amp;=  \basisFunction\left(\mappingMatrix_2\hiddenVector_{1}\right)\\
    \hiddenVector_{3} &amp;= \basisFunction\left(\mappingMatrix_3 \hiddenVector_{2}\right)\\
    \dataVector &amp;= \mappingVector_4 ^\top\hiddenVector_{3}
\end{align}
\]</span></p>
<p><span style="text-align:right"></span> ### Overfitting</p>
<ul>
<li><p>Potential problem: if number of nodes in two adjacent layers is big, corresponding <span class="math inline">\(\mappingMatrix\)</span> is also very big and there is the potential to overfit.</p></li>
<li><p>Proposed solution: “dropout”.</p></li>
<li><p>Alternative solution: parameterize <span class="math inline">\(\mappingMatrix\)</span> with its SVD. <span class="math display">\[
  \mappingMatrix = \eigenvectorMatrix\eigenvalueMatrix\eigenvectwoMatrix^\top
  \]</span> or <span class="math display">\[
  \mappingMatrix = \eigenvectorMatrix\eigenvectwoMatrix^\top
  \]</span> where if <span class="math inline">\(\mappingMatrix \in \Re^{k_1\times k_2}\)</span> then <span class="math inline">\(\eigenvectorMatrix\in \Re^{k_1\times q}\)</span> and <span class="math inline">\(\eigenvectwoMatrix \in \Re^{k_2\times q}\)</span>, i.e. we have a low rank matrix factorization for the weights.</p></li>
</ul>
</section>
<section id="low-rank-approximation" class="slide level3">
<h3>Low Rank Approximation</h3>
<object class="svgplot " align data="../slides/diagrams/wisuvt.svg" style="vertical-align:middle;">
</object>
<p><span style="text-align:right"></span></p>
</section>
<section id="deep-neural-network-2" class="slide level3">
<h3>Deep Neural Network</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/deep-nn-bottleneck1.svg" style="vertical-align:middle;">
</object>
</section>
<section id="deep-neural-network-3" class="slide level3">
<h3>Deep Neural Network</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/deep-nn-bottleneck2.svg" style="vertical-align:middle;">
</object>
</section>
<section id="mathematically-1" class="slide level3">
<h3>Mathematically</h3>
<p>The network can now be written mathematically as <span class="math display">\[
\begin{align}
  \latentVector_{1} &amp;= \eigenvectwoMatrix^\top_1 \inputVector\\
  \hiddenVector_{1} &amp;= \basisFunction\left(\eigenvectorMatrix_1 \latentVector_{1}\right)\\
  \latentVector_{2} &amp;= \eigenvectwoMatrix^\top_2 \hiddenVector_{1}\\
  \hiddenVector_{2} &amp;= \basisFunction\left(\eigenvectorMatrix_2 \latentVector_{2}\right)\\
  \latentVector_{3} &amp;= \eigenvectwoMatrix^\top_3 \hiddenVector_{2}\\
  \hiddenVector_{3} &amp;= \basisFunction\left(\eigenvectorMatrix_3 \latentVector_{3}\right)\\
  \dataVector &amp;= \mappingVector_4^\top\hiddenVector_{3}.
\end{align}
\]</span></p>
</section>
<section id="a-cascade-of-neural-networks" class="slide level3">
<h3>A Cascade of Neural Networks</h3>
<p><span class="math display">\[
\begin{align}
  \latentVector_{1} &amp;= \eigenvectwoMatrix^\top_1 \inputVector\\
  \latentVector_{2} &amp;= \eigenvectwoMatrix^\top_2 \basisFunction\left(\eigenvectorMatrix_1 \latentVector_{1}\right)\\
  \latentVector_{3} &amp;= \eigenvectwoMatrix^\top_3 \basisFunction\left(\eigenvectorMatrix_2 \latentVector_{2}\right)\\
  \dataVector &amp;= \mappingVector_4 ^\top \latentVector_{3}
\end{align}
\]</span></p>
</section>
<section id="cascade-of-gaussian-processes" class="slide level3">
<h3>Cascade of Gaussian Processes</h3>
<ul>
<li><p>Replace each neural network with a Gaussian process <span class="math display">\[
\begin{align}
  \latentVector_{1} &amp;= \mappingFunctionVector_1\left(\inputVector\right)\\
  \latentVector_{2} &amp;= \mappingFunctionVector_2\left(\latentVector_{1}\right)\\
  \latentVector_{3} &amp;= \mappingFunctionVector_3\left(\latentVector_{2}\right)\\
  \dataVector &amp;= \mappingFunctionVector_4\left(\latentVector_{3}\right)
\end{align}
\]</span></p></li>
<li><p>Equivalent to prior over parameters, take width of each layer to infinity.</p></li>
</ul>
<p><span style="text-align:right"></span></p>
<!-- No slide titles in this context -->
<p><span style="text-align:right"></span></p>
</section>
<section id="section-5" class="slide level3">
<h3></h3>
<p><span class="fragment fade-in"><small>Outline of the DeepFace architecture. A front-end of a single convolution-pooling-convolution filtering on the rectified input, followed by three locally-connected layers and two fully-connected layers. Color illustrates feature maps produced at each layer. The net includes more than 120 million parameters, where more than 95% come from the local and fully connected.</small></span></p>
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<p><span style="text-align:right"><small>Source: DeepFace <span class="citation" data-cites="Taigman:deepface14">(Taigman et al., 2014)</span></small></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="section-6" class="slide level3">
<h3></h3>
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/576px-Early_Pinball.jpg" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-7" class="slide level3">
<h3></h3>
<div style="text-align:center">
<object class="svgplot " align data="../slides/diagrams/pinball001.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="section-8" class="slide level3">
<h3></h3>
<div style="text-align:center">
<object class="svgplot " align data="../slides/diagrams/pinball002.svg" style="vertical-align:middle;">
</object>
</div>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="mathematically-2" class="slide level3">
<h3>Mathematically</h3>
<ul>
<li>Composite <em>multivariate</em> function</li>
</ul>
<p><span class="math display">\[
  \mathbf{g}(\inputVector)=\mappingFunctionVector_5(\mappingFunctionVector_4(\mappingFunctionVector_3(\mappingFunctionVector_2(\mappingFunctionVector_1(\inputVector))))).
  \]</span></p>
</section>
<section id="equivalent-to-markov-chain" class="slide level3">
<h3>Equivalent to Markov Chain</h3>
<ul>
<li>Composite <em>multivariate</em> function <span class="math display">\[
  p(\dataVector|\inputVector)= p(\dataVector|\mappingFunctionVector_5)p(\mappingFunctionVector_5|\mappingFunctionVector_4)p(\mappingFunctionVector_4|\mappingFunctionVector_3)p(\mappingFunctionVector_3|\mappingFunctionVector_2)p(\mappingFunctionVector_2|\mappingFunctionVector_1)p(\mappingFunctionVector_1|\inputVector)
  \]</span></li>
</ul>
<object class="svgplot " align data="../slides/diagrams/deepgp/deep-markov.svg" style="vertical-align:middle;">
</object>
</section>
<section id="section-9" class="slide level3">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/deep-markov-vertical.svg" style="vertical-align:middle;">
</object>
</section>
<section id="why-deep" class="slide level3">
<h3>Why Deep?</h3>
<ul>
<li><p>Gaussian processes give priors over functions.</p></li>
<li>Elegant properties:</li>
<li><p>e.g. <em>Derivatives</em> of process are also Gaussian distributed (if they exist).</p></li>
<li><p>For particular covariance functions they are ‘universal approximators’, i.e. all functions can have support under the prior.</p></li>
<li><p>Gaussian derivatives might ring alarm bells.</p></li>
<li><p>E.g. a priori they don’t believe in function ‘jumps’.</p></li>
</ul>
</section>
<section id="stochastic-process-composition" class="slide level3">
<h3>Stochastic Process Composition</h3>
<ul>
<li><p>From a process perspective: <em>process composition</em>.</p></li>
<li><p>A (new?) way of constructing more complex <em>processes</em> based on simpler components.</p></li>
</ul>
</section>
<section id="section-10" class="slide level3">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/deep-markov-vertical.svg" style="vertical-align:middle;">
</object>
</section>
<section id="section-11" class="slide level3">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/deep-markov-vertical-side.svg" style="vertical-align:middle;">
</object>
</section>
<section id="difficulty-for-probabilistic-approaches" class="slide level3">
<h3>Difficulty for Probabilistic Approaches</h3>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<object class="svgplot " align="center" data="../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg" style="vertical-align:middle;">
</object>
</section>
<section id="difficulty-for-probabilistic-approaches-1" class="slide level3">
<h3>Difficulty for Probabilistic Approaches</h3>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<object class="svgplot " align="center" data="../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg" style="vertical-align:middle;">
</object>
</section>
<section id="difficulty-for-probabilistic-approaches-2" class="slide level3">
<h3>Difficulty for Probabilistic Approaches</h3>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<object class="svgplot " align="center" data="../slides/diagrams/dimred/gaussian-through-nonlinear.svg" style="vertical-align:middle;">
</object>
<p><span style="text-align:right"></span></p>
</section>
<section id="deep-gaussian-processes" class="slide level3">
<h3>Deep Gaussian Processes</h3>
<ul>
<li>Deep architectures allow abstraction of features <span class="citation" data-cites="Bengio:deep09 Hinton:fast06 Salakhutdinov:quantitative08">(Bengio, 2009; Hinton and Osindero, 2006; Salakhutdinov and Murray, n.d.)</span></li>
<li>We use variational approach to stack GP models.</li>
</ul>
<p><span style="text-align:right"></span></p>
</section>
<section id="stacked-pca" class="slide level3">
<h3>Stacked PCA</h3>
<script>
showDivs(0, 'stack-pca-sample');
</script>
<small></small> <input id="range-stack-pca-sample" type="range" min="0" max="4" value="0" onchange="setDivs('stack-pca-sample')" oninput="setDivs('stack-pca-sample')">
<button onclick="plusDivs(-1, 'stack-pca-sample')">
❮
</button>
<button onclick="plusDivs(1, 'stack-pca-sample')">
❯
</button>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " align data="../slides/diagrams/stack-pca-sample-0.svg" style="vertical-align:middle;">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " align data="../slides/diagrams/stack-pca-sample-1.svg" style="vertical-align:middle;">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " align data="../slides/diagrams/stack-pca-sample-2.svg" style="vertical-align:middle;">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " align data="../slides/diagrams/stack-pca-sample-3.svg" style="vertical-align:middle;">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " align data="../slides/diagrams/stack-pca-sample-4.svg" style="vertical-align:middle;">
</object>
</div>
<p><span style="text-align:right"></span></p>
</section>
<section id="stacked-gp" class="slide level3">
<h3>Stacked GP</h3>
<script>
showDivs(0, 'stack-gp-sample');
</script>
<small></small> <input id="range-stack-gp-sample" type="range" min="0" max="4" value="0" onchange="setDivs('stack-gp-sample')" oninput="setDivs('stack-gp-sample')">
<button onclick="plusDivs(-1, 'stack-gp-sample')">
❮
</button>
<button onclick="plusDivs(1, 'stack-gp-sample')">
❯
</button>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " align data="../slides/diagrams/stack-gp-sample-0.svg" style="vertical-align:middle;">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " align data="../slides/diagrams/stack-gp-sample-1.svg" style="vertical-align:middle;">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " align data="../slides/diagrams/stack-gp-sample-2.svg" style="vertical-align:middle;">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " align data="../slides/diagrams/stack-gp-sample-3.svg" style="vertical-align:middle;">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " align data="../slides/diagrams/stack-gp-sample-4.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="analysis-of-deep-gps" class="slide level3">
<h3>Analysis of Deep GPs</h3>
<ul>
<li><p><em>Avoiding pathologies in very deep networks</em> <span class="citation" data-cites="Duvenaud:pathologies14">Duvenaud et al. (2014)</span> show that the derivative distribution of the process becomes more <em>heavy tailed</em> as number of layers increase.</p></li>
<li><p><em>How Deep Are Deep Gaussian Processes?</em> <span class="citation" data-cites="Dunlop:deep2017">Dunlop et al. (n.d.)</span> perform a theoretical analysis possible through conditional Gaussian Markov property.</p></li>
</ul>
</section>
<section id="section-12" class="slide level3">
<h3></h3>
<iframe width="1120" height="630" src="https://www.youtube.com/embed/XhIvygQYFFQ?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level3">
<h3>GPy: A Gaussian Process Framework in Python</h3>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level3">
<h3>GPy: A Gaussian Process Framework in Python</h3>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level3">
<h3>Features</h3>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="olympic-marathon-data" class="slide level3">
<h3>Olympic Marathon Data</h3>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level3">
<h3>Olympic Marathon Data</h3>
<div style="text-align:center">
<object class="svgplot " align data="../slides/diagrams/datasets/olympic-marathon.svg" style="vertical-align:middle;">
</object>
</div>
<p><span style="text-align:right"></span></p>
</section>
<section id="alan-turing" class="slide level3">
<h3>Alan Turing</h3>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</section>
<section id="probability-winning-olympics" class="slide level3">
<h3>Probability Winning Olympics?</h3>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had been held in 1946?</li>
</ul>
<!--

### 





<table><tr><td width="40%"><img class="" src="../slides/diagrams/turing-run.jpg" width="" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></td><td width="50%"><img class="" src="../slides/diagrams/turing-times.gif" width="" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></td></tr></table>

-->
</section>
<section id="olympic-marathon-data-gp" class="slide level3">
<h3>Olympic Marathon Data GP</h3>
<object class="svgplot " align data="../slides/diagrams/gp/olympic-marathon-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="deep-gp-fit" class="slide level3">
<h3>Deep GP Fit</h3>
<ul>
<li><p>Can a Deep Gaussian process help?</p></li>
<li><p>Deep GP is one GP feeding into another.</p></li>
</ul>
</section>
<section id="olympic-marathon-data-deep-gp" class="slide level3">
<h3>Olympic Marathon Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/olympic-marathon-deep-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="olympic-marathon-data-deep-gp-1" class="slide level3">
<h3>Olympic Marathon Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-samples.svg" style="vertical-align:middle;">
</object>
</section>
<section id="olympic-marathon-data-latent-1" class="slide level3">
<h3>Olympic Marathon Data Latent 1</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-layer-0.svg" style="vertical-align:middle;">
</object>
</section>
<section id="olympic-marathon-data-latent-2" class="slide level3">
<h3>Olympic Marathon Data Latent 2</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-layer-1.svg" style="vertical-align:middle;">
</object>
</section>
<section id="olympic-marathon-pinball-plot" class="slide level3">
<h3>Olympic Marathon Pinball Plot</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-pinball.svg" style="vertical-align:middle;">
</object>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="della-gatta-gene-data" class="slide level3">
<h3>Della Gatta Gene Data</h3>
<ul>
<li>Given given expression levels in the form of a time series from <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>.</li>
</ul>
</section>
<section id="della-gatta-gene-data-1" class="slide level3">
<h3>Della Gatta Gene Data</h3>
<div style="text-align:center">
<object class="svgplot " align data="../slides/diagrams/datasets/della-gatta-gene.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="gene-expression-example" class="slide level3">
<h3>Gene Expression Example</h3>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.</li>
</ul>
</section>
<section id="section-13" class="slide level3">
<h3></h3>
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/1471-2105-12-180_1.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180" class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
</section>
<section id="section-14" class="slide level3">
<h3></h3>
</section>
<section id="tp53-gene-data-gp" class="slide level3">
<h3>TP53 Gene Data GP</h3>
<object class="svgplot " align data="../slides/diagrams/gp/della-gatta-gene-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="tp53-gene-data-gp-1" class="slide level3">
<h3>TP53 Gene Data GP</h3>
<object class="svgplot " align data="../slides/diagrams/gp/della-gatta-gene-gp2.svg" style="vertical-align:middle;">
</object>
</section>
<section id="tp53-gene-data-gp-2" class="slide level3">
<h3>TP53 Gene Data GP</h3>
<object class="svgplot " align data="../slides/diagrams/gp/della-gatta-gene-gp3.svg" style="vertical-align:middle;">
</object>
</section>
<section id="multiple-optima" class="slide level3">
<h3>Multiple Optima</h3>
<object class="svgplot " align data="../slides/diagrams/gp/multiple-optima000.svg" style="vertical-align:middle;">
</object>
<!--
### Multiple Optima



<object class="svgplot " align="" data="../slides/diagrams/gp/multiple-optima001.svg" style="vertical-align:middle;"></object>-->
</section>
<section id="tp53-gene-data-deep-gp" class="slide level3">
<h3>TP53 Gene Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/della-gatta-gene-deep-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="tp53-gene-data-deep-gp-1" class="slide level3">
<h3>TP53 Gene Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/della-gatta-gene-deep-gp-samples.svg" style="vertical-align:middle;">
</object>
</section>
<section id="tp53-gene-data-latent-1" class="slide level3">
<h3>TP53 Gene Data Latent 1</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/della-gatta-gene-deep-gp-layer-0.svg" style="vertical-align:middle;">
</object>
</section>
<section id="tp53-gene-data-latent-2" class="slide level3">
<h3>TP53 Gene Data Latent 2</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/della-gatta-gene-deep-gp-layer-1.svg" style="vertical-align:middle;">
</object>
</section>
<section id="tp53-gene-pinball-plot" class="slide level3">
<h3>TP53 Gene Pinball Plot</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/della-gatta-gene-deep-gp-pinball.svg" style="vertical-align:middle;">
</object>
<p><span style="text-align:right"></span></p>
</section>
<section id="step-function-data" class="slide level3">
<h3>Step Function Data</h3>
<object class="svgplot " align data="../slides/diagrams/datasets/step-function.svg" style="vertical-align:middle;">
</object>
</section>
<section id="step-function-data-gp" class="slide level3">
<h3>Step Function Data GP</h3>
<object class="svgplot " align data="../slides/diagrams/gp/step-function-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="step-function-data-deep-gp" class="slide level3">
<h3>Step Function Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/step-function-deep-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="step-function-data-deep-gp-1" class="slide level3">
<h3>Step Function Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/step-function-deep-gp-samples.svg" style="vertical-align:middle;">
</object>
</section>
<section id="step-function-data-latent-1" class="slide level3">
<h3>Step Function Data Latent 1</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/step-function-deep-gp-layer-0.svg" style="vertical-align:middle;">
</object>
</section>
<section id="step-function-data-latent-2" class="slide level3">
<h3>Step Function Data Latent 2</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/step-function-deep-gp-layer-1.svg" style="vertical-align:middle;">
</object>
</section>
<section id="step-function-data-latent-3" class="slide level3">
<h3>Step Function Data Latent 3</h3>
</section>
<section id="slidesdiagramsdeepgpstep-function-deep-gp-layer-2.svg" class="slide level3">
<h3>../slides/diagrams/deepgp/step-function-deep-gp-layer-2.svg</h3>
</section>
<section id="step-function-data-latent-4" class="slide level3">
<h3>Step Function Data Latent 4</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/step-function-deep-gp-layer-3.svg" style="vertical-align:middle;">
</object>
<object class="svgplot " align data="../slides/diagrams/deepgp/step-function-deep-gp-layer-0.svg" style="vertical-align:middle;">
</object>
<object class="svgplot " align data="../slides/diagrams/deepgp/step-function-deep-gp-layer-1.svg" style="vertical-align:middle;">
</object>
<object class="svgplot " align data="../slides/diagrams/deepgp/step-function-deep-gp-layer-2.svg" style="vertical-align:middle;">
</object>
<object class="svgplot " align data="../slides/diagrams/deepgp/step-function-deep-gp-layer-3.svg" style="vertical-align:middle;">
</object>
</section>
<section id="step-function-pinball-plot" class="slide level3">
<h3>Step Function Pinball Plot</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/step-function-deep-gp-pinball.svg" style="vertical-align:middle;">
</object>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="motorcycle-helmet-data" class="slide level3">
<h3>Motorcycle Helmet Data</h3>
<object class="svgplot " align data="../slides/diagrams/datasets/motorcycle-helmet.svg" style="vertical-align:middle;">
</object>
</section>
<section id="motorcycle-helmet-data-gp" class="slide level3">
<h3>Motorcycle Helmet Data GP</h3>
<object class="svgplot " align data="../slides/diagrams/gp/motorcycle-helmet-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="motorcycle-helmet-data-deep-gp" class="slide level3">
<h3>Motorcycle Helmet Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/motorcycle-helmet-deep-gp.svg" style="vertical-align:middle;">
</object>
</section>
<section id="motorcycle-helmet-data-deep-gp-1" class="slide level3">
<h3>Motorcycle Helmet Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-samples.svg" style="vertical-align:middle;">
</object>
</section>
<section id="motorcycle-helmet-data-latent-1" class="slide level3">
<h3>Motorcycle Helmet Data Latent 1</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-layer-0.svg" style="vertical-align:middle;">
</object>
</section>
<section id="motorcycle-helmet-data-latent-2" class="slide level3">
<h3>Motorcycle Helmet Data Latent 2</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-layer-1.svg" style="vertical-align:middle;">
</object>
</section>
<section id="motorcycle-helmet-pinball-plot" class="slide level3">
<h3>Motorcycle Helmet Pinball Plot</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-pinball.svg" style="vertical-align:middle;">
</object>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="robot-wireless-ground-truth" class="slide level3">
<h3>Robot Wireless Ground Truth</h3>
<object class="svgplot " align data="../slides/diagrams/datasets/robot-wireless-ground-truth.svg" style="vertical-align:middle;">
</object>
</section>
<section id="robot-wifi-data" class="slide level3">
<h3>Robot WiFi Data</h3>
<object class="svgplot " align data="../slides/diagrams/datasets/robot-wireless-dim-1.svg" style="vertical-align:middle;">
</object>
</section>
<section id="robot-wifi-data-gp" class="slide level3">
<h3>Robot WiFi Data GP</h3>
<object class="svgplot " align data="../slides/diagrams/gp/robot-wireless-gp-dim-1.svg" style="vertical-align:middle;">
</object>
</section>
<section id="robot-wifi-data-deep-gp" class="slide level3">
<h3>Robot WiFi Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/robot-wireless-deep-gp-dim-1.svg" style="vertical-align:middle;">
</object>
</section>
<section id="robot-wifi-data-deep-gp-1" class="slide level3">
<h3>Robot WiFi Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/robot-wireless-deep-gp-samples-dim-1.svg" style="vertical-align:middle;">
</object>
</section>
<section id="robot-wifi-data-latent-space" class="slide level3">
<h3>Robot WiFi Data Latent Space</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/robot-wireless-ground-truth.svg" style="vertical-align:middle;">
</object>
<object class="svgplot " align data="../slides/diagrams/deepgp/robot-wireless-latent-space.svg" style="vertical-align:middle;">
</object>
<p><span style="text-align:right"></span></p>
</section>
<section id="motion-capture" class="slide level3">
<h3>Motion Capture</h3>
<ul>
<li>‘High five’ data.</li>
<li>Model learns structure between two interacting subjects.</li>
</ul>
</section>
<section id="shared-lvm" class="slide level3">
<h3>Shared LVM</h3>
<object class="svgplot " align data="../slides/diagrams/shared.svg" style="vertical-align:middle;">
</object>
</section>
<section id="section-15" class="slide level3">
<h3></h3>
<p><img class="negate" src="../slides/diagrams/deep-gp-high-five2.png" width="100%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></p>
<p><span style="text-align:right"></span></p>
<p><small><span style="text-align:right">Thanks to: Zhenwen Dai and Neil D. Lawrence</span></small></p>
</section>
<section id="section-16" class="slide level3">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/usps-digits-latent.svg" style="vertical-align:middle;">
</object>
</section>
<section id="section-17" class="slide level3">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/usps-digits-hidden-1-0.svg" style="vertical-align:middle;">
</object>
</section>
<section id="section-18" class="slide level3">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/usps-digits-hidden-2-0.svg" style="vertical-align:middle;">
</object>
</section>
<section id="section-19" class="slide level3">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/usps-digits-hidden-3-0.svg" style="vertical-align:middle;">
</object>
</section>
<section id="section-20" class="slide level3">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/usps-digits-hidden-4-0.svg" style="vertical-align:middle;">
</object>
</section>
<section id="section-21" class="slide level3">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/digit-samples-deep-gp.svg" style="vertical-align:middle;">
</object>
<p><span style="text-align:right"></span></p>
</section>
<section id="deep-health" class="slide level3">
<h3>Deep Health</h3>
<object class="svgplot " align data="../slides/diagrams/deep-health.svg" style="vertical-align:middle;">
</object>
</section>
<section id="from-nips-2017" class="slide level3">
<h3>From NIPS 2017</h3>
<ul>
<li><em>Gaussian process based nonlinear latent structure discovery in multivariate spike train data</em> <span class="citation" data-cites="Anqi:gpspike2017">Wu et al. (2017)</span></li>
<li><em>Doubly Stochastic Variational Inference for Deep Gaussian Processes</em> <span class="citation" data-cites="Salimbeni:doubly2017">Salimbeni and Deisenroth (2017)</span></li>
<li><em>Deep Multi-task Gaussian Processes for Survival Analysis with Competing Risks</em> <span class="citation" data-cites="Alaa:deep2017">Alaa and van der Schaar (2017)</span></li>
<li><em>Counterfactual Gaussian Processes for Reliable Decision-making and What-if Reasoning</em> <span class="citation" data-cites="Schulam:counterfactual17">Schulam and Saria (2017)</span></li>
</ul>
</section>
<section id="some-other-works" class="slide level3">
<h3>Some Other Works</h3>
<ul>
<li><em>Deep Survival Analysis</em> <span class="citation" data-cites="Ranganath-survival16">Ranganath et al. (2016)</span></li>
<li><em>Recurrent Gaussian Processes</em> <span class="citation" data-cites="Mattos:recurrent15">Mattos et al. (2015)</span></li>
<li><em>Gaussian Process Based Approaches for Survival Analysis</em> <span class="citation" data-cites="Saul:thesis2016">Saul (2016)</span></li>
</ul>
</section>
<section id="data-driven" class="slide level3">
<h3>Data Driven</h3>
<ul>
<li>Machine Learning: Replicate Processes through <em>direct use of data</em>.</li>
<li>Aim to emulate cognitive processes through the use of data.</li>
<li>Use data to provide new approaches in control and optimization that should allow for emulation of human motor skills.</li>
</ul>
</section>
<section id="process-emulation" class="slide level3">
<h3>Process Emulation</h3>
<ul>
<li>Key idea: emulate the process as a mathematical function.</li>
<li>Each function has a set of <em>parameters</em> which control its behavior.</li>
<li><em>Learning</em> is the process of changing these parameters to change the shape of the function</li>
<li>Choice of which class of mathematical functions we use is a vital component of our <em>model</em>.</li>
</ul>
<p><span style="text-align:right"></span></p>
</section>
<section id="emukit-playground" class="slide level3">
<h3>Emukit Playground</h3>
<ul>
<li><p>Work <a href="https://twitter.com/_AdamHirst">Adam Hirst</a>, Software Engineering Intern and Cliff McCollum.</p></li>
<li><p>Tutorial on emulation.</p></li>
</ul>
</section>
<section id="emukit-playground-1" class="slide level3">
<h3>Emukit Playground</h3>
<div style="text-align:center">
<a href="https://amzn.github.io/emukit-playground/"><img class="" src="../slides/diagrams/uq/emukit-playground.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></a>
</div>
</section>
<section id="emukit-playground-2" class="slide level3">
<h3>Emukit Playground</h3>
<div style="text-align:center">
<a href="https://amzn.github.io/emukit-playground/#!/learn/bayesian_optimization"><img class="negate" src="../slides/diagrams/uq/emukit-playground-bayes-opt.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></a>
</div>
</section>
<section id="uncertainty-quantification" class="slide level3">
<h3>Uncertainty Quantification</h3>
<ul>
<li>Deep nets are powerful approach to images, speech, language.</li>
<li>Proposal: Deep GPs may also be a great approach, but better to deploy according to natural strengths.</li>
</ul>
</section>
<section id="uncertainty-quantification-1" class="slide level3">
<h3>Uncertainty Quantification</h3>
<ul>
<li>Probabilistic numerics, surrogate modelling, emulation, and UQ.</li>
<li>Not a fan of AI as a term.</li>
<li>But we are faced with increasing amounts of <em>algorithmic decision making</em>.</li>
</ul>
</section>
<section id="ml-and-decision-making" class="slide level3">
<h3>ML and Decision Making</h3>
<ul>
<li>When trading off decisions: compute or acquire data?</li>
<li>There is a critical need for uncertainty.</li>
</ul>
</section>
<section id="uncertainty-quantification-2" class="slide level3">
<h3>Uncertainty Quantification</h3>
<blockquote>
<p>Uncertainty quantification (UQ) is the science of quantitative characterization and reduction of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known.</p>
</blockquote>
<ul>
<li>Interaction between physical and virtual worlds of major interest.</li>
</ul>
</section>
<section id="contrast" class="slide level3">
<h3>Contrast</h3>
<ul>
<li>Simulation in <em>reinforcement learning</em>.</li>
<li>Known as <em>data augmentation</em>.</li>
<li>Newer, similar in spirit, but typically ignores uncertainty.</li>
</ul>
</section>
<section id="example-formula-one-racing" class="slide level3">
<h3>Example: Formula One Racing</h3>
<ul>
<li><p>Designing an F1 Car requires CFD, Wind Tunnel, Track Testing etc.</p></li>
<li><p>How to combine them?</p></li>
</ul>
</section>
<section id="mountain-car-simulator" class="slide level3">
<h3>Mountain Car Simulator</h3>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/uq/mountaincar.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="car-dynamics" class="slide level3">
<h3>Car Dynamics</h3>
<p><span class="math display">\[\inputVector_{t+1} = \mappingFunction(\inputVector_{t},\textbf{u}_{t})\]</span></p>
<p>where <span class="math inline">\(\textbf{u}_t\)</span> is the action force, <span class="math inline">\(\inputVector_t = (p_t, v_t)\)</span> is the vehicle state</p>
</section>
<section id="policy" class="slide level3">
<h3>Policy</h3>
<ul>
<li>Assume policy is linear with parameters <span class="math inline">\(\boldsymbol{\theta}\)</span></li>
</ul>
<p><span class="math display">\[\pi(\inputVector,\theta)= \theta_0 + \theta_p p + \theta_vv.\]</span></p>
</section>
<section id="emulate-the-mountain-car" class="slide level3">
<h3>Emulate the Mountain Car</h3>
<ul>
<li>Goal is find <span class="math inline">\(\theta\)</span> such that</li>
</ul>
<p><span class="math display">\[\theta^* = arg \max_{\theta} R_T(\theta).\]</span></p>
<ul>
<li>Reward is computed as 100 for target, minus squared sum of actions</li>
</ul>
</section>
<section id="random-linear-controller" class="slide level3">
<h3>Random Linear Controller</h3>
<iframe src="../slides/diagrams/uq/mountain_car_random.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
</section>
<section id="best-controller-after-50-iterations-of-bayesian-optimization" class="slide level3">
<h3>Best Controller after 50 Iterations of Bayesian Optimization</h3>
<iframe src="../slides/diagrams/uq/mountain_car_simulated.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
</section>
<section id="data-efficient-emulation" class="slide level3">
<h3>Data Efficient Emulation</h3>
<ul>
<li><p>For standard Bayesian Optimization ignored <em>dynamics</em> of the car.</p></li>
<li><p>For more data efficiency, first <em>emulate</em> the dynamics.</p></li>
<li><p>Then do Bayesian optimization of the <em>emulator</em>.</p></li>
<li><p>Use a Gaussian process to model <span class="math display">\[\Delta v_{t+1} = v_{t+1} - v_{t}\]</span> and <span class="math display">\[\Delta x_{t+1} = p_{t+1} - p_{t}\]</span></p></li>
<li><p>Two processes, one with mean <span class="math inline">\(v_{t}\)</span> one with mean <span class="math inline">\(p_{t}\)</span></p></li>
</ul>
</section>
<section id="emulator-training" class="slide level3">
<h3>Emulator Training</h3>
<ul>
<li><p>Used 500 randomly selected points to train emulators.</p></li>
<li><p>Can make proces smore efficient through <em>experimental design</em>.</p></li>
</ul>
</section>
<section id="comparison-of-emulation-and-simulation" class="slide level3">
<h3>Comparison of Emulation and Simulation</h3>
<object class="svgplot " align data="../slides/diagrams/uq/emu_sim_comparison.svg" style="vertical-align:middle;">
</object>
</section>
<section id="data-efficiency" class="slide level3">
<h3>Data Efficiency</h3>
<ul>
<li><p>Our emulator used only 500 calls to the simulator.</p></li>
<li><p>Optimizing the simulator directly required 37,500 calls to the simulator.</p></li>
</ul>
</section>
<section id="best-controller-using-emulator-of-dynamics" class="slide level3">
<h3>Best Controller using Emulator of Dynamics</h3>
<iframe src="../slides/diagrams/uq/mountain_car_emulated.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
<p>500 calls to the simulator vs 37,500 calls to the simulator</p>
<p><span class="math display">\[\mappingFunction_i\left(\inputVector\right) = \rho\mappingFunction_{i-1}\left(\inputVector\right) + \delta_i\left(\inputVector \right)\]</span></p>
</section>
<section id="multi-fidelity-emulation" class="slide level3">
<h3>Multi-Fidelity Emulation</h3>
<p><span class="math display">\[\mappingFunction_i\left(\inputVector\right) = \mappingFunctionTwo_{i}\left(\mappingFunction_{i-1}\left(\inputVector\right)\right) + \delta_i\left(\inputVector \right),\]</span></p>
</section>
<section id="best-controller-with-multi-fidelity-emulator" class="slide level3">
<h3>Best Controller with Multi-Fidelity Emulator</h3>
<iframe src="../slides/diagrams/uq/mountain_car_multi_fidelity.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
<p>250 observations of high fidelity simulator and 250 of the low fidelity simulator</p>
<p><span style="text-align:right"></span></p>
</section>
<section id="emukit" class="slide level3">
<h3>Emukit</h3>
<ul>
<li>Work by Javier Gonzalez, Andrei Paleyes, Mark Pullin, Maren Mahsereci, Alex Gessner, Aaron Klein.</li>
<li>Available on <a href="https://github.com/amzn/emukit">Github</a></li>
<li>Example <a href="https://github.com/amzn/emukit/blob/develop/notebooks/Emukit-sensitivity-montecarlo.ipynb">sensitivity notebook</a>.</li>
</ul>
</section>
<section id="emukit-software" class="slide level3">
<h3>Emukit Software</h3>
<ul>
<li><em>Multi-fidelity emulation</em>: build surrogate models for multiple sources of information;</li>
<li><em>Bayesian optimisation</em>: optimise physical experiments and tune parameters ML algorithms;</li>
<li><em>Experimental design/Active learning</em>: design experiments and perform active learning with ML models;</li>
<li><em>Sensitivity analysis</em>: analyse the influence of inputs on the outputs</li>
<li><em>Bayesian quadrature</em>: compute integrals of functions that are expensive to evaluate.</li>
</ul>
<p><span style="text-align:right"></span></p>
<p><span style="text-align:right"></span></p>
</section>
<section id="mxfusion-modular-probabilistic-programming-on-mxnet" class="slide level3">
<h3>MXFusion: Modular Probabilistic Programming on MXNet</h3>
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/mxfusion.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
<a href="https://github.com/amzn/MXFusion" class="uri">https://github.com/amzn/MXFusion</a>
</center>
</section>
<section id="mxfusion" class="slide level3">
<h3>MxFusion</h3>
<table>
<tr>
<td width="70%">
<ul>
<li>Work by Eric Meissner and Zhenwen Dai.</li>
<li>Probabilistic programming.</li>
<li>Available on <a href="https://github.com/amzn/mxfusion">Github</a>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/mxfusion-logo.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table></li>
</ul>
</section>
<section id="mxfusion-1" class="slide level3">
<h3>MxFusion</h3>
<ul>
<li>Targeted at challenges we face in emulation.</li>
<li>Composition of Gaussian processes (Deep GPs)</li>
<li>Combining GPs with neural networks.</li>
<li>Example <a href="https://github.com/amzn/MXFusion/blob/master/examples/notebooks/ppca_tutorial.ipynb">PPCA Tutorial</a>.</li>
</ul>
</section>
<section id="why-another-framework" class="slide level3">
<h3>Why another framework?</h3>
<ul>
<li>Existing libraries had either:</li>
<li>Probabilistic modelling with rich, flexible models and universal inference or</li>
<li>Specialized, efficient inference over a subset of models</li>
</ul>
<p><strong>We needed both</strong></p>
</section>
<section id="key-requirements" class="slide level3">
<h3>Key Requirements</h3>
<ul>
<li>Integration with deep learning</li>
<li>Flexiblility</li>
<li>Scalability</li>
<li>Specialized inference and models support
<ul>
<li>Bayesian Deep Learning methods</li>
<li>Rapid prototyping and software re-use</li>
<li>GPUs, specialized inference methods</li>
</ul></li>
</ul>
</section>
<section id="modularity" class="slide level3">
<h3>Modularity</h3>
<ul>
<li>Specialized Inference</li>
<li>Composability (tinkerability)
<ul>
<li>Better leveraging of expert expertise</li>
</ul></li>
</ul>
</section>
<section id="what-does-it-look-like" class="slide level3">
<h3>What does it look like?</h3>
<p><strong>Modelling</strong></p>
<p><strong>Inference</strong></p>
</section>
<section id="modelling" class="slide level3">
<h3>Modelling</h3>
</section>
<section id="directed-graphs" class="slide level3">
<h3>Directed Graphs</h3>
<ul>
<li>Variable</li>
<li>Function</li>
<li>Distribution</li>
</ul>
</section>
<section id="example" class="slide level3">
<h3>Example</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">m <span class="op">=</span> Model()
m.mu <span class="op">=</span> Variable()
m.s <span class="op">=</span> Variable(transformation<span class="op">=</span>PositiveTransformation())
m.Y <span class="op">=</span> Normal.define_variable(mean<span class="op">=</span>m.mu, variance<span class="op">=</span>m.s)</code></pre></div>
</section>
<section id="primary-components-in-modeling" class="slide level3">
<h3>3 primary components in modeling</h3>
<ul>
<li>Variable</li>
<li>Distribution</li>
<li>Function</li>
</ul>
</section>
<section id="primary-methods-for-models" class="slide level3">
<h3>2 primary methods for models</h3>
<ul>
<li><code>log_pdf</code></li>
<li><code>draw_samples</code></li>
</ul>
</section>
<section id="inference-two-classes" class="slide level3">
<h3>Inference: Two Classes</h3>
<ul>
<li>Variational Inference</li>
<li>MCMC Sampling (<em>soon</em>) Built on MXNet Gluon (imperative code, not static graph)</li>
</ul>
</section>
<section id="example-1" class="slide level3">
<h3>Example</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">infr <span class="op">=</span> GradBasedInference(inference_algorithm<span class="op">=</span>MAP(model<span class="op">=</span>m, observed<span class="op">=</span>[m.Y]))
infr.run(Y<span class="op">=</span>data)</code></pre></div>
</section>
<section id="modules" class="slide level3">
<h3>Modules</h3>
<ul>
<li>Model + Inference together form building blocks.
<ul>
<li>Just doing modular modeling with universal inference doesn’t really scale, need specialized inference methods for specialized modelling objects like non-parametrics.</li>
</ul></li>
</ul>
</section>
<section id="long-term-aim" class="slide level3">
<h3>Long term Aim</h3>
<ul>
<li>Simulate/Emulate the components of the system.
<ul>
<li>Validate with real world using multifidelity.</li>
<li>Interpret system using e.g. sensitivity analysis.</li>
</ul></li>
<li>Perform end to end learning to optimize.
<ul>
<li>Maintain interpretability.</li>
</ul></li>
</ul>
</section>
<section id="acknowledgments" class="slide level3">
<h3>Acknowledgments</h3>
<p>Stefanos Eleftheriadis, John Bronskill, Hugh Salimbeni, Rich Turner, Zhenwen Dai, Javier Gonzalez, Andreas Damianou, Mark Pullin, Eric Meissner.</p>
</section>
<section id="thanks" class="slide level3">
<h3>Thanks!</h3>
<ul>
<li>twitter: @lawrennd</li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
</section>
<section id="references" class="slide level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Alaa:deep2017">
<p>Alaa, A.M., van der Schaar, M., 2017. Deep multi-task Gaussian processes for survival analysis with competing risks, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 2326–2334.</p>
</div>
<div id="ref-Alvarez:efficient10">
<p>Álvarez, M.A., Luengo, D., Titsias, M.K., Lawrence, N.D., 2010. Efficient multioutput Gaussian processes through variational inducing kernels, in:. pp. 25–32.</p>
</div>
<div id="ref-Bengio:deep09">
<p>Bengio, Y., 2009. Learning Deep Architectures for AI. Found. Trends Mach. Learn. 2, 1–127. <a href="https://doi.org/10.1561/2200000006" class="uri">https://doi.org/10.1561/2200000006</a></p>
</div>
<div id="ref-Thang:unifying17">
<p>Bui, T.D., Yan, J., Turner, R.E., 2017. A unifying framework for Gaussian process pseudo-point approximations using power expectation propagation. Journal of Machine Learning Research 18, 1–72.</p>
</div>
<div id="ref-Dai:gpu14">
<p>Dai, Z., Damianou, A., Hensman, J., Lawrence, N.D., 2014. Gaussian process models with parallelization and GPU acceleration.</p>
</div>
<div id="ref-Damianou:thesis2015">
<p>Damianou, A., 2015. Deep Gaussian processes and variational propagation of uncertainty (PhD thesis). University of Sheffield.</p>
</div>
<div id="ref-DellaGatta:direct08">
<p>Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D., Missero, C., Bernardo, D. di, 2008. Direct targets of the trp63 transcription factor revealed by a combination of gene expression profiling and reverse engineering. Genome Research 18, 939–948. <a href="https://doi.org/10.1101/gr.073601.107" class="uri">https://doi.org/10.1101/gr.073601.107</a></p>
</div>
<div id="ref-Dunlop:deep2017">
<p>Dunlop, M.M., Girolami, M.A., Stuart, A.M., Teckentrup, A.L., n.d. How deep are deep Gaussian processes? Journal of Machine Learning Research 19, 1–46.</p>
</div>
<div id="ref-Duvenaud:pathologies14">
<p>Duvenaud, D., Rippel, O., Adams, R., Ghahramani, Z., 2014. Avoiding pathologies in very deep networks, in:.</p>
</div>
<div id="ref-Gal:distributed14">
<p>Gal, Y., Wilk, M. van der, Rasmussen, C.E., n.d. Distributed variational inference in sparse Gaussian process regression and latent variable models, in:.</p>
</div>
<div id="ref-Hensman:bigdata13">
<p>Hensman, J., Fusi, N., Lawrence, N.D., n.d. Gaussian processes for big data, in:.</p>
</div>
<div id="ref-Hinton:fast06">
<p>Hinton, G.E., Osindero, S., 2006. A fast learning algorithm for deep belief nets. Neural Computation 18, 2006.</p>
</div>
<div id="ref-Hoffman:stochastic12">
<p>Hoffman, M., Blei, D.M., Wang, C., Paisley, J., 2012. Stochastic variational inference, arXiv preprint arXiv:1206.7051.</p>
</div>
<div id="ref-Kalaitzis:simple11">
<p>Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking differentially expressed gene expression time courses through Gaussian process regression. BMC Bioinformatics 12. <a href="https://doi.org/10.1186/1471-2105-12-180" class="uri">https://doi.org/10.1186/1471-2105-12-180</a></p>
</div>
<div id="ref-Lawrence:larger07">
<p>Lawrence, N.D., n.d. Learning for larger datasets with the Gaussian process latent variable model, in:. pp. 243–250.</p>
</div>
<div id="ref-MacKay:gpintroduction98">
<p>MacKay, D.J.C., n.d. Introduction to Gaussian processes, in:. pp. 133–166.</p>
</div>
<div id="ref-Mattos:recurrent15">
<p>Mattos, C.L.C., Dai, Z., Damianou, A.C., Forth, J., Barreto, G.A., Lawrence, N.D., 2015. Recurrent gaussian processes. CoRR abs/1511.06644.</p>
</div>
<div id="ref-Quinonero:unifying05">
<p>Quiñonero Candela, J., Rasmussen, C.E., 2005. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research 6, 1939–1959.</p>
</div>
<div id="ref-Ranganath-survival16">
<p>Ranganath, R., Perotte, A., Elhadad, N., Blei, D., 2016. Deep survival analysis, in: Doshi-Velez, F., Fackler, J., Kale, D., Wallace, B., Wiens, J. (Eds.), Proceedings of the 1st Machine Learning for Healthcare Conference, Proceedings of Machine Learning Research. PMLR, Children’s Hospital LA, Los Angeles, CA, USA, pp. 101–114.</p>
</div>
<div id="ref-Salakhutdinov:quantitative08">
<p>Salakhutdinov, R., Murray, I., n.d. On the quantitative analysis of deep belief networks, in:. pp. 872–879.</p>
</div>
<div id="ref-Salimbeni:doubly2017">
<p>Salimbeni, H., Deisenroth, M., 2017. Doubly stochastic variational inference for deep Gaussian processes, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 4591–4602.</p>
</div>
<div id="ref-Saul:thesis2016">
<p>Saul, A.D., 2016. Gaussian process based approaches for survival analysis (PhD thesis). University of Sheffield.</p>
</div>
<div id="ref-Schulam:counterfactual17">
<p>Schulam, P., Saria, S., 2017. Counterfactual Gaussian processes for reliable decision-making and what-if reasoning, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 1696–1706.</p>
</div>
<div id="ref-Seeger:auto17">
<p>Seeger, M.W., Hetzel, A., Dai, Z., Lawrence, N.D., 2017. Auto-differentiating linear algebra. CoRR abs/1710.08717.</p>
</div>
<div id="ref-Snelson:pseudo05">
<p>Snelson, E., Ghahramani, Z., n.d. Sparse Gaussian processes using pseudo-inputs, in:.</p>
</div>
<div id="ref-Taigman:deepface14">
<p>Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014. DeepFace: Closing the gap to human-level performance in face verification, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. <a href="https://doi.org/10.1109/CVPR.2014.220" class="uri">https://doi.org/10.1109/CVPR.2014.220</a></p>
</div>
<div id="ref-Titsias:variational09">
<p>Titsias, M.K., n.d. Variational learning of inducing variables in sparse Gaussian processes, in:. pp. 567–574.</p>
</div>
<div id="ref-Anqi:gpspike2017">
<p>Wu, A., Roy, N.G., Keeley, S., Pillow, J.W., 2017. Gaussian process based nonlinear latent structure discovery in multivariate spike train data, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 3499–3508.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
